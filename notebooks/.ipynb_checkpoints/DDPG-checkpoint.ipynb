{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float32_preprocessor(states):\n",
    "    \"\"\"\n",
    "    Convert list of states into the form suitable for model. By default we assume Variable\n",
    "\n",
    "    Args:\n",
    "        states: list of numpy arrays with states\n",
    "\n",
    "    Returns:\n",
    "        cleaned variable in the form of np.float32\n",
    "    \"\"\"\n",
    "\n",
    "    np_states = np.array(states, dtype=np.float32)\n",
    "    return torch.tensor(np_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGActor, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, act_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 400),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(400 + act_size, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetwork:\n",
    "    \"\"\"\n",
    "    Wrapper around model which provides copy of it instead of trained weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "    def sync(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def alpha_sync(self, alpha):\n",
    "        \"\"\"\n",
    "        Blend params of target net with params from the model\n",
    "        :param alpha:\n",
    "        \"\"\"\n",
    "        assert isinstance(alpha, float)\n",
    "        assert 0.0 < alpha <= 1.0\n",
    "        \n",
    "        state = self.model.state_dict()\n",
    "        tgt_state = self.target_model.state_dict()\n",
    "        for k, v in state.items():\n",
    "            tgt_state[k] = tgt_state[k] * alpha + (1 - alpha) * v\n",
    "\n",
    "        self.target_model.load_state_dict(tgt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, state_dims, action_dims, buffer=1000000, min_buffer=50000, batch=64):\n",
    "        self.buffer_size = buffer\n",
    "        self.min_buffer_size = min_buffer\n",
    "        self.state_dims = state_dims\n",
    "        self.action_dims = action_dims\n",
    "        self.batch_size = batch\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "\n",
    "        # preallocate memory\n",
    "        self.actions = np.empty((self.buffer_size,) + self.action_dims, dtype = np.float32)\n",
    "        self.rewards = np.empty(self.buffer_size, dtype = np.float32)\n",
    "        self.states = np.empty((self.buffer_size,) + self.state_dims, dtype = np.float32)\n",
    "        self.terminals = np.empty(self.buffer_size, dtype = np.bool)   \n",
    "        \n",
    "        self.state_batch = np.empty((self.batch_size,) + self.state_dims, dtype = np.float32)\n",
    "        self.next_state_batch = np.empty((self.batch_size,) + self.state_dims, dtype = np.float32)\n",
    "        \n",
    "        \n",
    "    def add(self, action, reward, state, terminal):        \n",
    "        assert state.shape == self.state_dims\n",
    "        assert action.shape == self.action_dims\n",
    "\n",
    "        self.actions[self.current, ...] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.states[self.current, ...] = state\n",
    "        self.terminals[self.current] = terminal\n",
    "        self.count = max(self.count, self.current + 1)\n",
    "        self.current = (self.current + 1) % self.buffer_size\n",
    "        \n",
    "  \n",
    "    def getState(self, index):\n",
    "        # Returns the state at position 'index'.\n",
    "        return self.states[index, ...]\n",
    "         \n",
    "\n",
    "    def getMinibatch(self):\n",
    "        # memory should be initially populated with random actions up to 'min_buffer_size'\n",
    "        assert self.count >= self.min_buffer_size, \"Replay memory does not contain enough samples to start learning, take random actions to populate replay memory\"\n",
    "                \n",
    "        # sample random indexes\n",
    "        indexes = []\n",
    "        # do until we have a full batch of states\n",
    "        while len(indexes) < self.batch_size:\n",
    "            # find random index \n",
    "            while True:\n",
    "                # sample one index\n",
    "                index = np.random.randint(1, self.count)\n",
    "                # check index is ok\n",
    "                # if state and next state wrap over current pointer, then get new one (as state from current pointer position will not be from same episode as state from previous position)\n",
    "                if index == self.current:\n",
    "                    continue\n",
    "                # if state and next state wrap over episode end, i.e. current state is terminal, then get new one (note that next state can be terminal)\n",
    "                if self.terminals[index-1]:\n",
    "                    continue\n",
    "                # index is ok to use\n",
    "                break\n",
    "            \n",
    "            # Populate states and next_states with selected state and next_state\n",
    "            # NB! having index first is fastest in C-order matrices\n",
    "            self.state_batch[len(indexes), ...] = self.getState(index - 1)\n",
    "            self.next_state_batch[len(indexes), ...] = self.getState(index)\n",
    "            indexes.append(index)   \n",
    "        \n",
    "        actions = self.actions[indexes]\n",
    "        rewards = self.rewards[indexes]\n",
    "        terminals = self.terminals[indexes]\n",
    "        \n",
    "        return self.state_batch, actions, rewards, self.next_state_batch, terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=0.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(env, replay_mem):\n",
    "    sys.stdout.write('\\nPopulating replay memory with random actions...\\n')   \n",
    "    sys.stdout.flush()          \n",
    "    env.reset()\n",
    "\n",
    "    for random_step in range(1, OBSERVATION+1):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminal, _ = env.step(action)\n",
    "        replay_mem.add(action, reward, state, terminal)\n",
    "\n",
    "        if terminal:\n",
    "            env.reset()\n",
    "\n",
    "        if random_step % 1000 == 0:\n",
    "            sys.stdout.write('\\x1b[2K\\rStep {:d}/{:d}'.format(random_step, OBSERVATION))\n",
    "            sys.stdout.flush() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_networks(device, act_net, crt_net, act_opt, crt_opt, replay_mem, gamma=0.99):\n",
    "\n",
    "    # Get minibatch\n",
    "    states_batch, actions_batch, rewards_batch, next_states_batch, terminals_batch = replay_mem.getMinibatch()\n",
    "    \n",
    "    \n",
    "    #preprocess \n",
    "    \n",
    "    states, actions, rewards, dones, last_states = [], [], [], [], []\n",
    "    \n",
    "    for s,a,r,s_,d in zip(states_batch, actions_batch, rewards_batch, next_states_batch, terminals_batch):\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(s_ is None)\n",
    "        if s_ is None:\n",
    "            last_states.append(s)\n",
    "        else:\n",
    "            last_states.append(s_)\n",
    "            \n",
    "            \n",
    "    states_v = float32_preprocessor(states).to(device)\n",
    "    actions_v = float32_preprocessor(actions).to(device)\n",
    "    rewards_v = float32_preprocessor(rewards).to(device)\n",
    "    last_states_v = float32_preprocessor(last_states).to(device)\n",
    "    dones_t = float32_preprocessor(dones).to(device)\n",
    "  \n",
    "    \n",
    "    # Critic training step \n",
    "    actions_next = tgt_act_net.target_model(last_states_v)\n",
    "    Q_targets_next = tgt_crt_net.target_model(last_states_v, actions_next)\n",
    "    \n",
    "    Q_targets = rewards_v + (gamma * Q_targets_next * (1 - dones_t))\n",
    "    \n",
    "    Q_expected = crt_net(states_v, actions_v)\n",
    "    \n",
    "    critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "    \n",
    "    crt_opt.zero_grad()\n",
    "    critic_loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(crt_net.parameters(), 1)\n",
    "    crt_opt.step()\n",
    "    \n",
    "#     crt_opt.zero_grad()\n",
    "#     # Predict actions for next states by passing next states through policy target network\n",
    "#     future_action = tgt_act_net.target_model(last_states_v)\n",
    "#     q_value = crt_net(states_v, actions_v)\n",
    "#     # Predict target Q values by passing next states and actions through value target network\n",
    "# #     future_Q = sess.run(critic_target.output, {state_ph:next_states_batch, action_ph:future_action})[:,0]\n",
    "#     future_Q = tgt_crt_net.target_model(last_states_v, future_action)\n",
    "#     # Q values of the terminal states is 0 by definition\n",
    "#     future_Q[dones_t] = 0\n",
    "#     targets = rewards_v + (future_Q * gamma)\n",
    "    \n",
    "#     critic_loss = F.mse_loss(q_value, future_Q)\n",
    "#     critic_loss.backward()\n",
    "#     crt_opt.step()\n",
    "    \n",
    "    \n",
    "    # Actor training step\n",
    "    actions_pred = act_net(states_v)\n",
    "    actor_loss = -crt_net(states_v, actions_pred).mean()\n",
    "    act_opt.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    act_opt.step()\n",
    "    \n",
    "#     act_opt.zero_grad()\n",
    "#     # Get policy network's action outputs for selected states\n",
    "#     actor_actions = act_net(states_v)\n",
    "#     actions_pred = act_net(states_v)\n",
    "#     actor_loss = -crt_net(states_v, actions_pred).mean()\n",
    "#     # Minimize the loss\n",
    "#     actor_loss.backward()\n",
    "#     act_opt.step()\n",
    "\n",
    "    tgt_act_net.alpha_sync(alpha=TAU)\n",
    "    tgt_crt_net.alpha_sync(alpha=TAU)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"msinto\"\n",
    "ENV = \"Pendulum-v0\"\n",
    "SEED = 99999999\n",
    "NOISE_SCALE = 0.1\n",
    "CUDA = False\n",
    "LRA = 0.0001\n",
    "LRC = 0.001\n",
    "RENDER = False\n",
    "OBSERVATION = 50000\n",
    "EXPLORATION = 1000\n",
    "SAVE_CP = 200\n",
    "TAU = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPGActor(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=300, out_features=1, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "DDPGCritic(\n",
      "  (obs_net): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (out_net): Sequential(\n",
      "    (0): Linear(in_features=401, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Environment\n",
    "env = gym.make(ENV)\n",
    "state_dims = env.observation_space.shape\n",
    "action_dims = env.action_space.shape\n",
    "action_bound_low = env.action_space.low\n",
    "action_bound_high = env.action_space.high\n",
    "\n",
    "# Set random seeds for reproducability\n",
    "env.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialise replay memory\n",
    "replay_mem = ReplayMemory(state_dims, action_dims)\n",
    "\n",
    "# Initialise Ornstein-Uhlenbeck Noise generator\n",
    "exploration_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dims))\n",
    "noise_scaling = NOISE_SCALE * (action_bound_high - action_bound_low)\n",
    "\n",
    "# Networks\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "\n",
    "act_net = DDPGActor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "crt_net = DDPGCritic(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "\n",
    "\n",
    "print(act_net)\n",
    "print(crt_net)\n",
    "\n",
    "tgt_act_net = TargetNetwork(act_net)\n",
    "tgt_crt_net = TargetNetwork(crt_net)\n",
    "\n",
    "# setup save directory\n",
    "save_path = os.path.join(\"saves\", \"ddpg-\" + NAME)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# network optimizers\n",
    "act_opt = optim.Adam(act_net.parameters(), lr=LRA)\n",
    "crt_opt = optim.Adam(crt_net.parameters(), lr=LRC)\n",
    "\n",
    "# Writer\n",
    "writer = SummaryWriter(comment=\"-ddpg_\" + NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Populating replay memory with random actions...\n",
      "Step 50000/50000[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\n",
      "\n",
      "Training...\n",
      "Episode 96/1000 \t Avg Reward = -1382.888 \t Reward = -1361.488 \t (0.021 s/step)[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K"
     ]
    }
   ],
   "source": [
    "# Initially populate replay memory by taking random actions \n",
    "observe(env, replay_mem)\n",
    "\n",
    "sys.stdout.write('\\n\\nTraining...\\n')   \n",
    "sys.stdout.flush()\n",
    "\n",
    "start_ep = 0\n",
    "total_rewards = []\n",
    "\n",
    "for train_ep in range(start_ep+1, EXPLORATION+1):      \n",
    "    # Reset environment and noise process\n",
    "    state = env.reset()\n",
    "    exploration_noise.reset()\n",
    "    \n",
    "    train_step = 0\n",
    "    episode_reward = 0\n",
    "    duration_values = []\n",
    "    ep_done = False\n",
    "    \n",
    "    \n",
    "#     sys.stdout.write('\\n')   \n",
    "#     sys.stdout.flush()\n",
    "\n",
    "    while not ep_done:\n",
    "        train_step += 1\n",
    "        start_time = time.time()            \n",
    "       \n",
    "        action = act_net(float32_preprocessor(state).to(device)).cpu().data.numpy()\n",
    "        action += exploration_noise() * noise_scaling\n",
    "        \n",
    "        state, reward, terminal, _ = env.step(action)\n",
    "        replay_mem.add(action, reward, state, terminal)\n",
    "        \n",
    "        episode_reward += reward \n",
    "            \n",
    "        update_networks(device, act_net, crt_net, act_opt, crt_opt, replay_mem)\n",
    "        \n",
    "        \n",
    "        # Display progress            \n",
    "        duration = time.time() - start_time\n",
    "        duration_values.append(duration)\n",
    "        ave_duration = sum(duration_values)/float(len(duration_values))\n",
    "        \n",
    "        \n",
    "            \n",
    "        if terminal:\n",
    "            total_rewards.append(episode_reward)\n",
    "            reward100 = total_rewards[-100:]\n",
    "            avg_reward = sum(reward100)/len(reward100)\n",
    "            sys.stdout.write('\\x1b[2K\\rEpisode {:d}/{:d} \\t Avg Reward = {:.3f} \\t Reward = {:.3f} \\t ({:.3f} s/step)'.format(train_ep, EXPLORATION, avg_reward, episode_reward, ave_duration))\n",
    "            sys.stdout.flush() \n",
    "            ep_done = True\n",
    "            \n",
    "    \n",
    "    if train_ep % SAVE_CP == 0:\n",
    "        name = \"ep_%d.dat\" % (train_ep)\n",
    "        fname = os.path.join(save_path, name)\n",
    "        torch.save(act_net.state_dict(), fname)\n",
    "        sys.stdout.write('\\n Checkpoint saved.')   \n",
    "        sys.stdout.flush() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
